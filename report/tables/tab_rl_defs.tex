\begin{longtable}{@{}r|Y@{}}
	\caption{Reinforcement Learning Terms and Definitions \cite{emami_2016}\cite{huang_2018}\cite{matiisen_2015}\cite{sutton_2017}} 		\label{tab:rl_defs}\\
	\toprule
	Term & \multicolumn{1}{c}{Definition} \\
	\midrule 
	\endfirsthead
	\toprule
	Term & \multicolumn{1}{c}{Definition} \\
	\midrule 
	\endhead
	Action & Performed by the actor in the environment to move to the next state. \\
	Actor/Agent & Chooses actions based on a policy. \\
	Credit assignment problem & The challenge in determining which action was responsible for the long term reward. \\
	Critic & Evaluates the policy. \\
	Discount factor ($\gamma$)& A lower discount factor reduces the value of long-term rewards, favoring more immediate rewards. \\
	Environment & The various states and associated rewards through which the actor navigates. \\
	Episode & A sequence of actions, states, and rewards from initial to terminal state. \\
	Experience replay & Training technique in which transitions sampled randomly from past transitions are used to update the network in order to break up data correlation and stabilize convergence. \\
	Feature & Numerical-valued input to an artificial neural network. \\
	Greedy & A policy which chooses actions to maximize the immediate next reward. \\
	Learning rate & A hyper-parameter determining how quickly network weights are adjusted. \\
	Model-based & Algorithm learns a model of the environment and optimizes its policy based on the model. \\
	Model-free & Algorithm optimizes its policy without requiring a model of the environment. \\
	Policy ($\pi$) & The strategy by which the actor chooses its actions, e.g. random, exploratory, greedy. \\
	Off-policy & Values are learned based on a policy independent from the one used by the actor. \\
	On-policy & Values are learned based on the current policy used by the actor. \\
	Reinforcement learning & Training examples are accompanied by time-delayed rewards, actor learns to maximize its long-term reward in an environment. \\
	Reward/return (r) & A number emitted by the environment denoting "goodness" of the state. \\
	Supervised learning & Training examples are accompanied by the desired label or outcome, actor learns to produce the correct label, classification. \\
	State (s) & A representation of the situation in the environment. \\
	Transition & A set of state, action, reward, and next state for a single time step. \\
	Unsupervised learning & Training examples do not have associated labels, actor finds hidden patterns within data, clustering.  \\
	Q-value (Q) & The expected long-term discounted reward of a state and action. \\
	Value (V) & The expected long-term discounted reward of a state. \\
	\bottomrule 
\end{longtable} 
